/**
 * ðŸ¦ž OpenClaw API Endpoint â€” GOAT Royalty
 * Handles local LLM communication, model management, and gateway status
 * Â© 2025 Harvey Miller / FASTASSMAN Publishing Inc
 */

const OLLAMA_DEFAULT_URL = process.env.OLLAMA_URL || 'http://localhost:11434';

export default async function handler(req, res) {
  // CORS headers
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');

  if (req.method === 'OPTIONS') {
    return res.status(200).end();
  }

  const { action } = req.query;

  try {
    switch (action) {
      // â•â•â• Gateway Status â•â•â•
      case 'status':
        return res.status(200).json({
          success: true,
          gateway: {
            status: 'online',
            version: 'OpenClaw v2026.2.26',
            uptime: process.uptime(),
            platform: 'GOAT Royalty Integration',
            ollamaUrl: OLLAMA_DEFAULT_URL,
          },
          system: {
            nodeVersion: process.version,
            platform: process.platform,
            arch: process.arch,
            memory: {
              total: Math.round(process.memoryUsage().heapTotal / 1024 / 1024) + 'MB',
              used: Math.round(process.memoryUsage().heapUsed / 1024 / 1024) + 'MB',
            }
          }
        });

      // â•â•â• List Available Models â•â•â•
      case 'models':
        try {
          const ollamaRes = await fetch(`${OLLAMA_DEFAULT_URL}/api/tags`, {
            signal: AbortSignal.timeout(5000)
          });
          if (ollamaRes.ok) {
            const data = await ollamaRes.json();
            return res.status(200).json({
              success: true,
              source: 'ollama-live',
              models: data.models || []
            });
          }
        } catch (e) {
          // Ollama not running â€” return registry defaults
        }
        return res.status(200).json({
          success: true,
          source: 'registry-fallback',
          models: [
            { name: 'llama3.3:70b', size: 40000000000, modified_at: new Date().toISOString() },
            { name: 'llama3.2:3b', size: 2000000000, modified_at: new Date().toISOString() },
            { name: 'mistral:7b', size: 4100000000, modified_at: new Date().toISOString() },
            { name: 'mixtral:8x7b', size: 26000000000, modified_at: new Date().toISOString() },
            { name: 'codellama:34b', size: 19000000000, modified_at: new Date().toISOString() },
            { name: 'deepseek-coder-v2:16b', size: 8900000000, modified_at: new Date().toISOString() },
            { name: 'phi-3:medium', size: 7900000000, modified_at: new Date().toISOString() },
            { name: 'gemma2:27b', size: 16000000000, modified_at: new Date().toISOString() },
          ],
          note: 'Ollama not detected â€” showing registry models. Run: ollama serve'
        });

      // â•â•â• Chat Completion â•â•â•
      case 'chat':
        if (req.method !== 'POST') {
          return res.status(405).json({ error: 'POST required for chat' });
        }

        const { model, messages, temperature, max_tokens } = req.body;

        // Try Ollama first
        try {
          const chatRes = await fetch(`${OLLAMA_DEFAULT_URL}/api/chat`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              model: model || 'llama3.2:3b',
              messages: messages || [],
              stream: false,
              options: {
                temperature: temperature || 0.7,
                num_predict: max_tokens || 4096,
              }
            }),
            signal: AbortSignal.timeout(120000)
          });

          if (chatRes.ok) {
            const data = await chatRes.json();
            return res.status(200).json({
              success: true,
              source: 'ollama-live',
              message: data.message,
              model: data.model,
              eval_count: data.eval_count,
              eval_duration: data.eval_duration,
            });
          }
        } catch (e) {
          // Ollama not available â€” use smart fallback
        }

        // Smart fallback responses for demo mode
        const lastMessage = messages?.[messages.length - 1]?.content || '';
        const fallbackResponse = generateSmartResponse(lastMessage);

        return res.status(200).json({
          success: true,
          source: 'goat-ai-fallback',
          message: {
            role: 'assistant',
            content: fallbackResponse
          },
          model: 'goat-royalty-ai',
          note: 'Running in demo mode. Install Ollama for full local LLM: curl -fsSL https://ollama.com/install.sh | sh'
        });

      // â•â•â• Pull Model â•â•â•
      case 'pull':
        if (req.method !== 'POST') {
          return res.status(405).json({ error: 'POST required' });
        }
        const { modelName } = req.body;
        try {
          const pullRes = await fetch(`${OLLAMA_DEFAULT_URL}/api/pull`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ name: modelName, stream: false }),
            signal: AbortSignal.timeout(600000) // 10 min timeout for large models
          });
          if (pullRes.ok) {
            const data = await pullRes.json();
            return res.status(200).json({ success: true, ...data });
          }
        } catch (e) {
          return res.status(503).json({
            success: false,
            error: 'Ollama not available. Start with: ollama serve',
            command: `ollama pull ${modelName}`
          });
        }
        break;

      // â•â•â• Health Check â•â•â•
      case 'health':
        let ollamaOnline = false;
        try {
          const healthRes = await fetch(`${OLLAMA_DEFAULT_URL}/`, {
            signal: AbortSignal.timeout(3000)
          });
          ollamaOnline = healthRes.ok;
        } catch (e) {}

        return res.status(200).json({
          success: true,
          gateway: 'online',
          ollama: ollamaOnline ? 'connected' : 'offline',
          timestamp: new Date().toISOString(),
          app: 'GOAT Royalty Ã— OpenClaw',
          version: '1.0.0'
        });

      default:
        return res.status(200).json({
          success: true,
          service: 'ðŸ¦ž OpenClaw API â€” GOAT Royalty',
          endpoints: {
            'GET /api/openclaw?action=status': 'Gateway status & system info',
            'GET /api/openclaw?action=models': 'List available LLM models',
            'GET /api/openclaw?action=health': 'Health check (Ollama + Gateway)',
            'POST /api/openclaw?action=chat': 'Send chat message to local LLM',
            'POST /api/openclaw?action=pull': 'Pull/download a model via Ollama',
          }
        });
    }
  } catch (error) {
    console.error('OpenClaw API Error:', error);
    return res.status(500).json({
      success: false,
      error: error.message || 'Internal server error'
    });
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Smart Fallback Response Generator (when Ollama is not running)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function generateSmartResponse(input) {
  const lower = input.toLowerCase();

  if (lower.includes('royalt') || lower.includes('earning') || lower.includes('revenue')) {
    return `ðŸ“Š **GOAT Royalty Analysis**\n\nBased on the FASTASSMAN Publishing catalog of 3,650+ tracks:\n\nâ€¢ **Estimated Monthly Revenue**: $12,400 - $18,600 across all platforms\nâ€¢ **Top Platform**: Spotify (42%), Apple Music (28%), YouTube Music (15%)\nâ€¢ **Growth Trend**: +8.3% quarter-over-quarter\nâ€¢ **Top Performing Track**: Currently analyzing streaming data...\n\nðŸ’¡ *For real-time calculations, connect to the Royalty Engine at /api/royalty-engine*\n\nðŸ¦ž *Running in demo mode â€” install Ollama for full AI analysis: \`curl -fsSL https://ollama.com/install.sh | sh\`*`;
  }

  if (lower.includes('track') || lower.includes('song') || lower.includes('music') || lower.includes('catalog')) {
    return `ðŸŽµ **Catalog Overview â€” FASTASSMAN Publishing**\n\nâ€¢ **Total Tracks**: 3,650+\nâ€¢ **Active Platforms**: 25+ DSPs worldwide\nâ€¢ **Genres**: Hip-Hop, R&B, Electronic, Pop, Gospel\nâ€¢ **Key Artists**: DJ Speedy, Harvey Miller, featured collaborations\nâ€¢ **ISRC Coverage**: 100% registered\nâ€¢ **Publishing Admin**: ASCAP/BMI registered\n\nI can help you search specific tracks, analyze performance, or generate reports. What would you like to explore?\n\nðŸ¦ž *OpenClaw Demo Mode â€” connect Ollama for deep catalog analysis*`;
  }

  if (lower.includes('hello') || lower.includes('hi') || lower.includes('hey') || lower.includes('what can')) {
    return `ðŸ‘‘ **Welcome to GOAT Royalty AI â€” Powered by OpenClaw**\n\nI'm your personal AI assistant for managing the FASTASSMAN Publishing empire. Here's what I can help with:\n\nðŸŽµ **Music Catalog** â€” Search, analyze, and manage 3,650+ tracks\nðŸ’° **Royalty Tracking** â€” Real-time earnings across all platforms\nðŸ“Š **Analytics** â€” Streaming trends, audience insights, growth metrics\nðŸŽ¬ **Content** â€” Video generation, artwork, marketing materials\nðŸ”’ **Privacy** â€” All processing runs locally on your hardware\n\nWhat would you like to work on today?\n\nðŸ¦ž *Tip: Install Ollama + pull llama3.3:70b for the most powerful local AI experience*`;
  }

  if (lower.includes('deploy') || lower.includes('server') || lower.includes('host')) {
    return `ðŸš€ **Deployment Guide â€” GOAT Royalty**\n\n**Current Setup:**\nâ€¢ Web App: Next.js on Hostinger VPS\nâ€¢ Desktop: Electron (SuperGOATRoyalty)\nâ€¢ AI Backend: OpenClaw + Ollama\n\n**Quick Deploy Commands:**\n\`\`\`bash\n# Deploy web app\nsh DEPLOY-ALL-IN-ONE.sh\n\n# Start OpenClaw gateway\nopenclaw gateway --port 18789\n\n# Start Ollama\nollama serve\n\n# Pull recommended model\nollama pull llama3.3:70b\n\`\`\`\n\nNeed help with a specific deployment step?`;
  }

  return `ðŸ¦ž **OpenClaw AI Assistant**\n\nI received your message: "${input.substring(0, 100)}${input.length > 100 ? '...' : ''}"\n\nI'm currently running in **demo mode** without a local LLM backend. To unlock full AI capabilities:\n\n1. **Install Ollama**: \`curl -fsSL https://ollama.com/install.sh | sh\`\n2. **Pull a model**: \`ollama pull llama3.2:3b\` (fast) or \`ollama pull llama3.3:70b\` (powerful)\n3. **Start the gateway**: The app will auto-detect Ollama on localhost:11434\n\nOnce connected, I can provide intelligent responses powered by local LLMs with full privacy â€” no data leaves your machine.\n\nðŸ‘‘ *GOAT Royalty Ã— OpenClaw â€” Your Music Empire, Your AI*`;
}